#!/bin/bash
start=`date +%s`
hadoop jar /usr/local/hadoop-1.2.1/contrib/streaming/hadoop-streaming-1.2.1.jar -input step1input -output step1output -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/shared/1410402685245taxonomy.xml#1410402685245taxonomy.xml -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/shared/1410402685245Jurkat_highLC_Frac14-16.mgf.xml#1410402685245Jurkat_highLC_Frac14-16.mgf.xml -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/shared/mrtandem#mrtandem -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/shared/default_input.xml#default_input.xml -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/shared/missense_snv_protein_40.fa#missense_snv_protein_40.fa -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/shared/Jurkat_highLC_Frac14-16.mgf#Jurkat_highLC_Frac14-16.mgf -mapper "mrtandem -mapper1_1 hdfs://172.31.25.188:9000/user/ubuntu/ 1410402685245Jurkat_highLC_Frac14-16.mgf.xml" -reducer "mrtandem -reducer1_1 hdfs://172.31.25.188:9000/user/ubuntu/ 1410402685245Jurkat_highLC_Frac14-16.mgf.xml" -jobconf mapred.task.timeout=36000000 -jobconf mapred.reduce.tasks=1 -jobconf mapred.map.tasks=1 -jobconf mapred.reduce.tasks.speculative.execution=false -jobconf mapred.map.tasks.speculative.execution=false
stop=`date +%s`
echo "*******Step 1 running time: $[ stop - start ]s*******"
start=`date +%s`
hadoop jar /usr/local/hadoop-1.2.1/contrib/streaming/hadoop-streaming-1.2.1.jar -input step1output -output step2output -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/shared/1410402685245taxonomy.xml#1410402685245taxonomy.xml -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/shared/1410402685245Jurkat_highLC_Frac14-16.mgf.xml#1410402685245Jurkat_highLC_Frac14-16.mgf.xml -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/shared/mrtandem#mrtandem -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/shared/default_input.xml#default_input.xml -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/shared/missense_snv_protein_40.fa#missense_snv_protein_40.fa -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/shared/Jurkat_highLC_Frac14-16.mgf#Jurkat_highLC_Frac14-16.mgf -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/reducer1_1#reducer1_1 -mapper "mrtandem -mapper2_1 hdfs://172.31.25.188:9000/user/ubuntu/ 1410402685245Jurkat_highLC_Frac14-16.mgf.xml" -reducer "mrtandem -reducer2_1 hdfs://172.31.25.188:9000/user/ubuntu/ 1410402685245Jurkat_highLC_Frac14-16.mgf.xml" -jobconf mapred.task.timeout=36000000 -jobconf mapred.reduce.tasks=1 -jobconf mapred.map.tasks=1 -jobconf mapred.reduce.tasks.speculative.execution=false -jobconf mapred.map.tasks.speculative.execution=false
stop=`date +%s`
echo "*******Step 2 running time: $[ stop - start ]s*******"
start=`date +%s`
hadoop jar /usr/local/hadoop-1.2.1/contrib/streaming/hadoop-streaming-1.2.1.jar -input step2output -output step3output -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/shared/1410402685245taxonomy.xml#1410402685245taxonomy.xml -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/shared/1410402685245Jurkat_highLC_Frac14-16.mgf.xml#1410402685245Jurkat_highLC_Frac14-16.mgf.xml -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/shared/mrtandem#mrtandem -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/shared/default_input.xml#default_input.xml -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/shared/missense_snv_protein_40.fa#missense_snv_protein_40.fa -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/shared/Jurkat_highLC_Frac14-16.mgf#Jurkat_highLC_Frac14-16.mgf -cacheFile hdfs://172.31.25.188:9000/user/ubuntu/reducer2_1#reducer2_1 -mapper "mrtandem -mapper3_1 hdfs://172.31.25.188:9000/user/ubuntu/ 1410402685245Jurkat_highLC_Frac14-16.mgf.xml" -reducer "mrtandem -reducer3_1 hdfs://172.31.25.188:9000/user/ubuntu/ 1410402685245Jurkat_highLC_Frac14-16.mgf.xml -reportURL hdfs://172.31.25.188:9000/user/ubuntu/" -jobconf mapred.task.timeout=36000000 -jobconf mapred.reduce.tasks=1 -jobconf mapred.map.tasks=1 -jobconf mapred.reduce.tasks.speculative.execution=false -jobconf mapred.map.tasks.speculative.execution=false
stop=`date +%s`
echo "*******Step 3 running time: $[ stop - start ]s*******"
hadoop dfs -copyToLocal output output
start=`date +%s`
java -Xmx2048m -jar post_process/mzidentml-lib-1.6.10.jar Tandem2mzid output output.mzid -outputFragmentation false -decoyRegex "###REV###" -databaseFileFormatID MS:1001348 -massSpecFileFormatID MS:1001062 -idsStartAtZero false -compress false
java -Xmx2048m -jar post_process/mzidentml-lib-1.6.10.jar FalseDiscoveryRate output.mzid output_fdr.mzid -decoyRegex "###REV###" -decoyValue 1 -cvTerm MS:1001330 -betterScoresAreLower true -compress false
java -Xmx2048m -jar post_process/mzidentml-lib-1.6.10.jar Threshold output_fdr.mzid result.mzid -isPSMThreshold true -cvAccessionForScoreThreshold MS:1002354 -threshValue 0.01 -betterScoresAreLower true -deleteUnderThreshold true -compress false
python upload_data.py AKIAIWNERGLUEYZL7N7Q 2vg5/PqUH1DGRTi1ONYRXwf9lfrV6Mblf2vFIb4U capercloud-output result.mzid
stop=`date +%s`
echo "*******Post processing time: $[ stop - start ]s*******"
